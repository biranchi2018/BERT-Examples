{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biranchi2018/BERT-Examples/blob/master/1.Bert-for-TF2%20Toxic%20Comments%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK3krsA8KRIp",
        "colab_type": "code",
        "outputId": "c40b7ccb-e382-4abd-ae1e-ae22ad370055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 2.1MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=e94acfcceab463fbf250ea7843d524f93ca9b8c61cb7feabcfef3c68416ba247\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=36e0ba5c4c88e9d014a541267d56140e6d4c5a84966ff4ceac44533413f32c39\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=dd1d7cda4e9b1f23becc4475c55d9db321f28649dab760c28e555107dc395214\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LNI7arEPIPx",
        "colab_type": "code",
        "outputId": "bdb1e1e0-4e3b-41d1-fde3-4721e24457cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 3.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56enlVerOhEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Ox_h10OhGx",
        "colab_type": "code",
        "outputId": "e798491c-e9d1-4e3b-ff0f-1696aad92228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import bert\n",
        "from tensorflow.keras.models import  Model\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.2.0\n",
            "Hub version:  0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMOOiIVmP-3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n",
        "\n",
        "MAX_SEQ_LEN=128\n",
        "input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
        "                                       name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
        "                                   name=\"input_mask\")\n",
        "segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n",
        "                                    name=\"segment_ids\")\n",
        "\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXYcDM53V05P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "out = tf.keras.layers.Dense(6, activation=\"sigmoid\", name=\"dense_output\")(x)\n",
        "\n",
        "model = tf.keras.models.Model(\n",
        "      inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6CHF8WfV5gg",
        "colab_type": "code",
        "outputId": "49c517da-73c9-44ba-c0d9-c2b940f4a1b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 768)          0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           global_average_pooling1d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dense_output (Dense)            (None, 6)            4614        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,486,855\n",
            "Trainable params: 109,486,854\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQbqZQhXUTkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
        "\n",
        "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "\n",
        "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n",
        "\n",
        "tokenizer=FullTokenizer(vocab_file,do_lower_case)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9IaN5_Fkt7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_masks(tokens, max_seq_length):\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BsauL7dkt_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrE6fz8oOhOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"username\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"kagglekey\" # key from the json file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1JR2oSMOhRZ",
        "colab_type": "code",
        "outputId": "a3da6547-06a8-4a31-8561-6ce18de89268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        }
      },
      "source": [
        "!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.39M [00:00<?, ?B/s]\n",
            "100% 1.39M/1.39M [00:00<00:00, 97.6MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 98% 23.0M/23.4M [00:01<00:00, 14.2MB/s]\n",
            "100% 23.4M/23.4M [00:01<00:00, 14.7MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 68% 18.0M/26.3M [00:00<00:00, 27.5MB/s]\n",
            "100% 26.3M/26.3M [00:00<00:00, 31.9MB/s]\n",
            "Downloading test_labels.csv.zip to /content\n",
            "  0% 0.00/1.46M [00:00<?, ?B/s]\n",
            "100% 1.46M/1.46M [00:00<00:00, 199MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4MFikjLOhW1",
        "colab_type": "code",
        "outputId": "82cb2f36-2050-421e-a1fa-43cc4f19b5aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!unzip train.csv.zip\n",
        "!unzip test.csv.zip\t\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  test.csv.zip\n",
            "  inflating: test.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hao1rgFmOhT6",
        "colab_type": "code",
        "outputId": "509e2483-186d-4c1d-bb40-186980b562a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\t\t   test.csv\t test_labels.csv.zip  train.csv.zip\n",
            "sample_submission.csv.zip  test.csv.zip  train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn_0yNpsOhZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df=pd.read_csv('train.csv')\n",
        "\n",
        "df = df.sample(frac=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6hQ0xkqOhcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sentences = df[\"comment_text\"].fillna(\"CVxTz\").values\n",
        "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "train_y = df[list_classes].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZgPWUhcxKO4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "5ab4d2ff-15bd-459f-f06b-ed4909cbd733"
      },
      "source": [
        "train_sentences[:2]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"Someone should move the quotes to wikiquote, I would but I don't know how.\",\n",
              "       'Why is linking the province any different if it is beside a city? I still do not see the crime? The user when clicking on the link can see the underline to see if it is on the one link or separate. Also, why have you fixed only Flo Rida, but not the others?'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Rn2Sm5xQ0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d11b15e4-0bc5-4682-8e99-1d6c318a0ba2"
      },
      "source": [
        "train_y[:2]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Z-ZcEhsECa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_single_input(sentence,MAX_LEN):\n",
        "  \n",
        "  stokens = tokenizer.tokenize(sentence)\n",
        "  \n",
        "  stokens = stokens[:MAX_LEN]\n",
        "  \n",
        "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        " \n",
        "  ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n",
        "  masks = get_masks(stokens, MAX_SEQ_LEN)\n",
        "  segments = get_segments(stokens, MAX_SEQ_LEN)\n",
        "\n",
        "  return ids,masks,segments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnX-lYMrOheq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_input_array(sentences):\n",
        "\n",
        "  input_ids, input_masks, input_segments = [], [], []\n",
        "\n",
        "  for sentence in tqdm(sentences,position=0, leave=True):\n",
        "  \n",
        "    ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n",
        "\n",
        "    input_ids.append(ids)\n",
        "    input_masks.append(masks)\n",
        "    input_segments.append(segments)\n",
        "\n",
        "  return [np.asarray(input_ids, dtype=np.int32), \n",
        "            np.asarray(input_masks, dtype=np.int32), \n",
        "            np.asarray(input_segments, dtype=np.int32)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OenQWBJEvx9F",
        "colab_type": "code",
        "outputId": "2b712982-30f0-4fe0-d01b-51cfccdebf50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "inputs=create_input_array(train_sentences)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 159571/159571 [03:02<00:00, 876.27it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea7EhuD2xXlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "33092e32-4e4a-4a50-80fd-9e636bd1df66"
      },
      "source": [
        "inputs[:2]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[  101,  2619,  2323, ...,     0,     0,     0],\n",
              "        [  101,  2339,  2003, ...,     0,     0,     0],\n",
              "        [  101,  2562,  2033, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101, 23561,  1024, ...,     0,     0,     0],\n",
              "        [  101,  1000,  1010, ...,     0,     0,     0],\n",
              "        [  101,  1000,  1024, ...,  2011,  9530,   102]], dtype=int32),\n",
              " array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 1, 1, 1]], dtype=int32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZT3munMvw44",
        "colab_type": "code",
        "outputId": "ca00ad4b-6372-4b73-9217-ce1924838a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "model.fit(inputs,train_y,epochs=1,batch_size=32,validation_split=0.2,shuffle=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2152/3990 [===============>..............] - ETA: 2:20:11 - loss: 0.1465 - accuracy: 0.8538"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ebdcb1669c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAah8tMgosYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coUa-kxPoskX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "57032e36-8f0f-48a3-c3bf-12ac80849bfe"
      },
      "source": [
        "test_df=pd.read_csv(\"test.csv\")\n",
        "\n",
        "test_sentences = test_df[\"comment_text\"].fillna(\"CVxTz\").values\n",
        "\n",
        "test_sentences[110:150]\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"== Second issue == \\n\\n I'm thinking we need a taxonomy section; there are things that aren't really as clear as they maybe could be. Why is/what specific findings about the fossils places R. narmadensis in the subfamily Carnotaurinae? Why 'princely lizard of the Narmada'; is that significant as to the stature/behavior/predatorial prowess of R. narmadensis? If there is anything more precise on its taxonomy, such as that, it would be great to add.\",\n",
              "       '\" \\n\\n == Hey man == \\n\\n  \\n I saw you removing all the fair use images from Jimi Hendrix discography. It\\'s a bitch to do it, but good job doing the right thing. Right now I\\'m going through a lot of artist and discography pages looking for album covers to get rid of and other bad images. You got my back. Keep up the good work.++++ \"',\n",
              "       \": But there's only one EP and one album, which is really also an EP. Even in the 70s, it would have been considered an EP, although it could have been considered an album by length in the 50s or the 60s. This is my point. They don't have two full-length albums.\",\n",
              "       '\" \\n :Hi Olly, by moderator do you mean administrator?  There all no official requirements needed to become an administrator, however, the Wikipedia community must decide in favor of your request in order to be granted the rights.  Therefore, in order to have a successful request, you must show experience and trustworthiness throughout your edits over a period of time. This period of time has not set length, but more experience is favored over less.  I hope this helps, \\xa0 \"',\n",
              "       'পরিচয়   \\n\\n নাম-মোঃফরহাদ ইসলাম \\n পিতা-মোঃ শাহজাহান মিয়া \\n মাতা-মোছাঃফাহিমা বেগম \\n গ্রামঃ জানকিপুর সলই মন্ডল পাড়া \\n ইউনিয়নঃনিলাখিয়া \\n উপজেলাঃবকশীগঞ্জ \\n জেলাঃজামালপুর  \\n বিভাগঃঢাকা \\n\\n  \\n     ।।।।।।।।।।।।।।। \\n\\n  \\n\\n আমি ১৯৯৭ সালের ২০.জানুয়ারি আমার নানার বাড়িতে জন্মগ্রহণ করি। ২০০০ সালে নিজ গ্রামের স্কুলে ভর্তি হয় এবং ওই স্কুল থেকেই প্রাইমারি লেবেল শেষ করি।২০০৬ সালে ভর্তি নিজ ইউনিয়নের নিলাখিয়া আর জে পাইলট উচ্চ বিদ্যালয়ে ৬ষ্ঠ শ্রেনীতে এবং সেখানে ক্লাস এইট পর্যন্ত পড়ি।। পরে ভর্তি হই বকশীগঞ্জ টি.ভি.আই  এ সেখান থেকে ২০১২ সালে এস এস সি পাশ করি।তার পর টেক্সটাইল এ ডিপ্লোমা করার জন্য ভর্তি হলাম শহীদ আব্দুর রব সেরনিয়াবাত টেক্সটাইল ইঞ্জিনিয়ারিং কলেজ,বরিশাল। বর্তমানে আমি এই কলেজে ৬ষ্ঠ সেমিস্টার এর এক জন ছাত্র।।',\n",
              "       \"== scholarly corroboration of folklore == \\n\\n My understanding is that folklore can provide clues to history, medicine, and other areas of scholarship that can be verified, in part, through archaeology, biology, and so on, but I don't have any sources handy or for which I recall titles. If anyone does, please add a section to the article about scholarly confirmation. \\n\\n We could do the same for disconfirmation, but there's plenty on that in many places, usually in the form of debunking based on a lack of scientific evidence, which is a weaker standard than positive evidence. \\n\\n Thanks.\",\n",
              "       '==Redirects== \\n トレイン・トレイン, トレイン トレイン, トレイントレイン, トレイン-トレイン, トレイン+トレイン, トレイン*トレイン, トレイン, トレイン, トレィン☆トレィン, Torein・Torein, Torein Torein, Torein-Torein, Torein+Torein, Torein*Torein, Torein, Torein, Torein☆Torein, Train・Train, Train☆Train, トレイン—トレイン, Torein—Torein, Train—Train, トレイン–トレイン, Torein–Torein, Train–Train \\n\\n should redirect here',\n",
              "       '\"::I respect your point of view, and when this discussion originated on 8th April I would have tended to agree with you. However today this is now being mentioned by multiple expert sources. The opinion of F1 driver Pastor Maldonado is \"\"The rules are the same for everyone, so you need to avoid incidents ... But at the same time [the threat of penalties means] you cannot race, you need to only stay on track and wait for problems.\"\" (original article is behind paywall on Autosport). This issue was also discussed and recognized by expert commentators during Sky Sports F1 Chinese GP Free Practice 1. I will be adding this section back to the article with appropriate references.    \\n\\n \"',\n",
              "       'i would like to take a moment of my time to apologize for leaving my workstation long enough for an idiot to use my IP to be an idiot... it wont happen again, i promise. im sure he meant this hyde, who, as it turns out, is male. he has been explained to why that was vandalism, and why obsessing over Jrock is silly.',\n",
              "       'السلام عليكم و رحمة الله و بركاته الا الجميع \\n تفضلوا جميعا',\n",
              "       '\"   \\n\\n ::::::::::::::Another priceless gem from yourself demonstrating your general cluelessness: ,\"\"The fuse rating (and therefore the plug rating in the case of a non-rewireable plug) is determined by the cable attached, not the appliance it will be used with.\"\". Where on earth did you dig that nonsense up from? There are plenty of good examples around, but medium to large switch mode power supplies are often supplied with nominally 3 Amp cable in a moulded on plug, many with an IEC 60320 C7 type plug (rated by that standard at 2.5 Amps). According to you, the plug should therefore be fitted with a 3 Amp fuse (determined by the cable). If you did so, you would never get any but the smallest power supplies running. You have to fit a 5 Amp fuse because the very large >40 Amp momentary inrush current lasts long enough that it will blow a 3 Amp fuse. The cable and plug can retain their original ratings because the inrush current will not harm them. The rating of the fuse is determined by much much more than the cable size alone. Indeed, the type of power supply that is built into a plug usually has no fuse at all!   17:17\"',\n",
              "       '\"This is just bizarre. Ani Medjool\\'s abuse of wikipedia rules gets rewarded, and then he awards you a barnstar.  WTF?  Anyhow a new person, adept at ridicule, has joined in the conversation at falafel.  Sigh...   \\n\\n \"',\n",
              "       \":::They're good enough that they can no longer be considered for speedy deletion. The rationales are acceptable; the actual use may still be an issue needing review, and that I would take to FFD if necessary.\",\n",
              "       '\" \\n\\n :Understandable. I\\'ve had a lot of problems with him in the past, too, particularly in regard to copyright violations and, more recently, grammar and style. But, as far as this issue goes: What, do you just want me to block both of you for 24 hours? — \\'\\'\\'\\'\\'\\'  \"',\n",
              "       '\" \\n\\n Firstly still making undiffable edits and refusing to use edit summaries is becoming a behavioural issue that will need to be escalated if it doesn\\'t stop. Gun, I really don\\'t understand why you find it so hard to follow the basic standards of behaviour required to edit in a collaborative environment. \\n\\n Secondly, \"\"announcing\"\" changes on someone\\'s talk page doesn\\'t really count as discussing them collaboratively, rather it means you discussed it with me. \\n\\n Furthermore I told you to go to the NPOV noticeboard to raise the State University vs University issue if you wanted to discuss the issue, which you a) failed to do, b) you removed the disputed tag from the article without a result (as I suggested waiting until later), c) you made yet another diff which contained a large number of other unknown, unknowable and undisucssed changes to the article.   <> \"',\n",
              "       '\" \\n\\n ==Name in the introductory paragraph== \\n I\\'ve always been told that using \"\"Dr.\"\" & \"\"Ph.D.\"\" in the same name was incorrect, as the Ph.D. automatically conferred the status of doctor.   However, I\\'m not sure what to do in this case.  Since he is so well-known as \"\"Dr. Martin Luther King, Jr.,\"\" the normal approach of dropping \"\"Dr.\"\" and keeping \"\"Ph.D.\"\" doesn\\'t seem to be the best answer, even though it\\'s the one most style guides seem to endorse.  Would someone more intimately familliar with the article like to take a swing at changing it, or suggesting an alternative?   \"',\n",
              "       'to fuck you and ur family',\n",
              "       \"== Mrpainkiller, Neurophyte and Jean-Philippe == \\n\\n After a complaint at intervention against vandalism, I protected the article VampireFreaks.com.  It was either that, or block all of you for disruption and personal attacks.  I will unprotect the article in the morning.  In the meantime, please consider the following: \\n *Reverting another user's talk page is pretty rude.  There is a policy conflict between the user talk page policy, which says users can blank or archive warnings (although archiving is preferred by the community) and the vandalism policy, which says that removing warnings given to you is vandalism.  I don't agree with the vandalism point of view, and I haven't seen a lot of support for it lately except among people who are fighting amongst themselves for other reasons.  If someone deletes a warning they've obviously seen it.  If you are concerned about someone's edits, use a verbose edit summary so that even if the warning is removed the edit summary will be visible. \\n *I understand there is a view among two of you that Mrpainkiller7's edits are detrimental to the article.  There are lots of right ways to deal with this, including editing by consensus, Request for comment on the article or the user, Mediation, and third opinion.  I'm not going to judge the content, but fighting (wherever it occurs) is the wrong way to deal with the problem. \\n *Personal attacks are not to be tolerated.  Please deal with the content of edits, not character.  \\n\\n I'm leaving this message with each of you.  If you keep fighting over talk pages, I suppose I or another admin will eventually figure it's time to block someone.  Unless you are positive that you are 100% right and the other guy is 100% wrong, you should probably try and work this out peacefully in case the admins don't see things your way.\",\n",
              "       '== hrthrtdghrsdtghtrsdhtrhdgthjrtgh == \\n\\n fffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffhngviudfhgoiuerhgukfhkjgeruighervyfdohgobvijgfjgojfdhgjofhglkfhboerhgbdhoifdtyklerbhlkjhbdjgbojtgofgjrfjhlrkhilebotrjbh9jtrohibjrtlhujty;junmoytijoryjhigfjhgfjnifgjgfhkljtgilhjfdojgfdojgfkjngfdlkjgnklfgikfvihgifbfhbgkufbdsukfgglkdhvureuhgiurhgh4irtgyrehgui4t7iryggoifhgo54oerygtjeiygut8gopfe8gig89fjhg9r8tyr8hygb7hruegfigust8g0r0retugrijtrir6tihviurhgnviurwt,kcnguiergvknfgioerpjgklfupgrojgoirutkdhgl;rehgljkgyuorihfgiurwyhoug6y3oi4hg98w6u jtgnkjryt9jgoidug059jgiufd7teurjg98etujoijfgiosdug 0945ojtg0o8reuyjurfi ug945uj ygoip5uj5tg;eoiu495yp;w8y5urtg9ui8hui9g85w4mftmuv89tny0uvg8ytihguihfdkjvhu4krngkfdjs hgkjfghoeruygrhy goh tgoiugkiughgvhghghghhgghhghghghghghghghhhhhgtherkgheefjgsefdghh',\n",
              "       'HI FRANKIE HOW ARE YOU I HEARD YOU LIKE TO SUCK FAT DICK AND PUT IT IN YOUR EAR WHOLES',\n",
              "       '\"=Outside input=== \\n *I\\'ve given this debate a bit of a read through, and for the most part, it seems clear that although the actual content of many of  may be important, they are presented in an extremely PoV manner that IMO smells agenda-pushing. Issues (ii) and (vii) below are prime examples of that, though the rest work as well. The key here that I think GoldDragon needs to understand (and trust me, I have experience with POV pushers) is that Wikipedia is not a place to campaign, all additions to the article need to be entirely factual and presented in a clearly neutral manner - something that  seems to have done well. GoldDragon\\'s edits (specifically the ones identified as contentious in this discussion) should be reworked to be presented in a neutral manner, or else removed. In addition, this issue of what is notable enough for inclusion has come up often in my own disputes. The most useful tool I\\'ve come up with to deal with this is to ask myself \"\"Will the average, a-political Torontonian find this information useful?\"\" Things like the election sign business for example; while interesting to those who are intensely involved, if you were to ask the average person, they\\'d probably say something along the lines of \"\"Who cares?\"\" That, I find, is a good measure of notability when dealing with trivia in politician articles   \\n\\n ==\"',\n",
              "       '== black mamba == \\n\\n It.is ponious snake of the word and but it not kills many people but king cobra kills many people in India',\n",
              "       \"2005 (UTC) \\n ::::: Fuck you, Jim. Everyone knows I'm female. IJzeren Jan, Jim is harassing me on the basis of a medical chromosomal condition I was born with.  15:46, August 10,\",\n",
              "       \"==September 30, 2005== \\n Back to your old tricks, eh? Stop it or you'll be blocked.\",\n",
              "       '= Tomahawk Chop = \\n Need to add a section to talk about the bone-in ribeye steak, which is referred to as a Tomahawk Chop.',\n",
              "       '\" \\n :::::@yash read wp:VANDAL \\'\\'\\'\\'\\'\\' \"',\n",
              "       'u suck u suck u suck u suck u sucku suck u suck u suck u suck u u suck u suck u suck u suck u sucksucku suck u suck u suck u suck u sucku suck u suck uu suck u suck u suck u suck u suck suck u suck u sucku suck u suck u suck u suck u suck \\n u suck u suck u suck u suck u sucku suck u suck u suck u suck u u suck u suck u suck u suck u sucksucku suck u suck u suck u suck u sucku suck u suck uu suck u suck u suck u suck u suck suck u suck u sucku suck u suck u suck u suck u suck \\n u suck u suck u suck u suck u sucku suck u suck u suck u suck u u suck u suck u suck u suck u sucksucku suck u suck u suck u suck u sucku suck u suck uu suck u suck u suck u suck u suck suck u suck u sucku suck u suck u suck u suck u suck \\n u suck u suck u suck u suck u sucku suck u suck u suck u suck u u suck u suck u suck u suck u sucksucku suck u suck u suck u suck u sucku suck u suck uu suck u suck u suck u suck u suck suck u suck u sucku suck u suck u suck u su \\n u suck u suck u suck u suck u sucku suck u suck u suck u suck u u suck u suck u suck u suck u sucksucku suck u suck u suck u suck u sucku suck u suck uu suck u suck u suck u suck u suck suck u suck u sucku suck u suck u suck u suck u suckck u suck',\n",
              "       \"Slavery is forbidden in Islam?  That seems to run counter to history.  Weren't the Mamluks, simply to mention one group, former slaves?\",\n",
              "       '\":Perhaps WP:AN can provide some advice... I\\'m not quite sure what to do either.  —  •  •   \\n\\n \"',\n",
              "       '\"I notice that you and jehockman have close connection on your talk page. Jehockman edits SEO articles and runs an SEO company(conflict of interest? That is the fox guarding the chicken coup?). Perhaps you should have this sock puppet conversation with jonathan hockman. In my opinion having an SEO masquerade as a wiki editor and using sock puppets to delete other SEO\\'s is gamesmanship and clearly not in the spirit of providing accurate information on wikipedia but rather suppressing it. I just Googled \"\"seo expert\"\" and out of 5 million results Mr. Guy is number 3. Not notable? What is your ajenda?  \\n\\n \"',\n",
              "       '\" \\n ::No problem. Sorry my question was unclear - do you prefer the three arks picture in the article now, or the log drive photo (linked above) that probably is on Pine Creek - agree Little Pine is too little now. No idea if it was enlarged by a splash dam though, but I agree it looks more Pine than Little Pine.   \\'\\'\\'\\'\\'\\' \"',\n",
              "       \"Please do move my comment there to legitimize everything. I really don't know how to do things here. all i know is just to edit.\",\n",
              "       \":::Looks like this was an old issue so I took the liberty of removing it. I don't see any difference between the non-spaced version and the spaced version now. The spaced version just makes the intro look very strange.\",\n",
              "       'I saw the cricket match played on 2nd oct 15 at dharamshala india. i just wanted to know why our bowlers do not through dot-balls. our bowlers always throw short balls; i could not understand as to why short balls on which the opponent team scored easy runs. why they do not practise for dot-balls; instead of throwing balls with speed of 147 km/hr they should throw dot-balls say with speed of 130 km/hr so that they may not score easy runs.  \\n\\n My idea is that out of 120 balls 100 were short balls.  \\n\\n please pay special attention on this  \\n\\n i am lay man but click on such things, which are very much',\n",
              "       '**I just revamped WP:FART, I thought a little housekeeping would give it more street cred.',\n",
              "       '\" \\n\\n ==US== \\n I\\'m trying not to just be a whinger and a hand waver here, but it annoys me that there\\'s an entire section devoted to left vs. right politics in the US, yet there\\'s nothing similar regarding the rest of the world. The US bias of wikipedia is very irritating sometimes, though it shouldn\\'t be, as most Internet users are in the U.S. In other words, i\\'m a whiny European who is too lazy to write a non US section. \\n\\n From the US>  That\\'s easy to fix.  Start writing more about non-US views on topics, creating topics, and  getting your friends to help.  Something that is created by users being called biased only reflects the predominant users posting and thus can easily be remedied. \\n\\n Also from the US> No, it\\'s NOT so easy to fix. Throughout Wikipedia you will see entries from (obviously) American authors who have no concept of a world view and automatically write from a US perspective without any attempt to qualify their language. What, therefore, tends to happen is that the US view becomes a default with international views living in ghetto-ized paragraphs.     \\n\\n Also from the US> That is actually kind of the problem with Wikipedia all around in the first place, and that is that popular ideas are not necessarily correct. You can have people posting information about the corruptibility of whatever organization, but then the organization can have its people change the entry the next day.  —Preceding unsigned comment added by     \\n\\n  \\n This entire dicussion lends a great deal of credibilty to the idea that the left-right spectrum poorly represents political and economic views.  It seems to me that most of the debate below is equivalent to attempting to define the square root of negative 1 with only real numbers at our disposal.  It is painfully obvious that politics is muli-dimensional.  The most that can be hoped for here is to describe how the left-right spectrum is used and leave it at that.  This should probably be a very short article.  —Preceding unsigned comment added by     \\n\\n == Better Chart == \\n\\n I think that this page needs a metter chart. Like this one http://studentnewsdaily.com/conservative_vs_liberal_beliefs.shtml    \\n\\n  \\n Edit  \\n\\n I actually think that the article, in fact the whole political movement, needs a better definition of \"\"left-right\"\". Most political parties cannot adhere to a single category, including revolutionnary movements and far-right groups, as described somewhere else in the discussion. \\n\\n I suggest a new use of the \"\"left-right\"\" compass. \\n\\n The political compass uses the \"\"square\"\" system with two axis  x and y (social and économic), which is not very accurate because I know my political ideas, and they tend to be pretty nearer to Mussolini than to Gandhi. Whatever. \\n\\n What I suggest is a cube! \\n\\n 3 axis  x, y and z \\n\\n Économic \\n Social \\n Politic \\n\\n The first one defines the market system (if any) of a political group. \\n The second defines the social standing of a political group about the welfare of the people (welfare checks/charity, private/public school/medical system, etc) \\n The thirds one is, basically, the way a society treats it\\'s people (rehabilitation/punishment, democracy/totaliratism (in which I include Soviet Communism!) etc) \\n\\n What do you think?  —Preceding unsigned comment added by     \\n\\n == Interesting book == \\n\\n I came across this book: Left and Right in Global Politics \\n by Alain Noël and Jean-Philippe Thérien.  I haven\\'t read it yet but it seems to be of great interest.    \\n\\n == Removing unneccessary detail == \\n\\n I am removing several paragraphs that are unhelpful to the article: \\n * Edmund Burke - terms left and right not used in UK then \\n * Adam Smith - terms left and r',\n",
              "       ':Does J.P. Harris have anything to say about the Ancre?',\n",
              "       '\" \\n ::People don\\'t need to be sent messages. They can go to the talk page and see it for themselves.—  \"',\n",
              "       'REDIRECT Talk:Pieter Melvill van Carnbee (1743-1826)',\n",
              "       '\" \\n\\n Make all the personal attacks you want. Plus I read your replies & my so called \"\"Spamming\"\", & noticed of the 5 adimns I contacted, 2 replied in my support, while the other 3 didn\\'t reply at all. You are the only one who has a problem. See here to fix it, ), just a little joke of course, or is it?...Mysterious pause.... Gooooooooodbye buddy, may out paths never meet again....  \"'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Cy1iusNqai5",
        "colab_type": "code",
        "outputId": "991659c1-c6c9-4c4a-b862-6fcbf6944789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "test_inputs=create_input_array(test_sentences[110:150])\n",
        "\n",
        "print(model.predict(test_inputs))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:00<00:00, 648.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.09538716 0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.0953871  0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538704 0.00635716 0.05412844 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538713 0.00635719 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.0953871  0.00635716 0.05412835 0.00132161 0.04550296 0.01215172]\n",
            " [0.09538716 0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538716 0.00635716 0.05412832 0.00132161 0.04550308 0.01215181]\n",
            " [0.09538716 0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.0953871  0.00635716 0.05412844 0.00132161 0.04550296 0.01215178]\n",
            " [0.0953871  0.00635716 0.05412841 0.00132161 0.04550296 0.01215172]\n",
            " [0.09538713 0.00635716 0.05412835 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538701 0.00635716 0.05412841 0.00132161 0.04550302 0.01215172]\n",
            " [0.0953871  0.00635716 0.05412844 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538698 0.00635716 0.05412841 0.00132161 0.0455029  0.01215172]\n",
            " [0.09538713 0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538716 0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538701 0.00635719 0.05412841 0.00132164 0.04550302 0.01215172]\n",
            " [0.09538713 0.00635719 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538728 0.00635719 0.05412817 0.00132164 0.04550296 0.01215181]\n",
            " [0.09538698 0.00635716 0.05412841 0.00132161 0.04550296 0.01215172]\n",
            " [0.09538716 0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538698 0.00635719 0.05412841 0.00132161 0.04550296 0.01215172]\n",
            " [0.09538704 0.00635716 0.05412844 0.00132155 0.04550296 0.01215178]\n",
            " [0.0953871  0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538704 0.00635719 0.05412841 0.00132161 0.04550296 0.01215172]\n",
            " [0.09538716 0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538716 0.00635719 0.05412841 0.00132161 0.04550302 0.01215178]\n",
            " [0.09538698 0.00635719 0.05412841 0.00132164 0.04550299 0.01215172]\n",
            " [0.0953871  0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538716 0.00635719 0.05412844 0.00132161 0.04550296 0.01215178]\n",
            " [0.0953871  0.00635716 0.05412844 0.00132161 0.04550296 0.01215178]\n",
            " [0.0953871  0.00635716 0.05412844 0.00132161 0.04550302 0.01215178]\n",
            " [0.0953871  0.00635716 0.05412844 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538716 0.00635716 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538701 0.00635716 0.05412841 0.00132161 0.04550296 0.01215172]\n",
            " [0.09538716 0.00635719 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538698 0.00635716 0.05412841 0.00132161 0.04550296 0.01215172]\n",
            " [0.09538698 0.00635716 0.05412841 0.00132161 0.0455029  0.01215172]\n",
            " [0.09538713 0.00635719 0.05412841 0.00132161 0.04550296 0.01215178]\n",
            " [0.09538713 0.00635719 0.05412841 0.00132161 0.04550296 0.01215178]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kOuKe7JIxHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYrXIrE6IxE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FVU_UAqIxKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jluVwdCIxOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OKM2UX4IxM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqDI3jwSIxC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAeK2LimIwwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}